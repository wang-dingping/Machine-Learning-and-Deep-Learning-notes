# 机器学习-周志华

[toc]



## 第一章 绪论

## 1.2 基本术语

|                                      |                                          |
| ------------------------------------ | ---------------------------------------- |
| 数据集                               | 样本/示例/特征向量                       |
| 属性/特征                            | 属性值                                   |
| 样本空间                             | 训练样本                                 |
| 标记 label                           | 样例                                     |
| 标记空间 label space/输出空间        | 分类 classification → 预测的结果为离散值 |
| 回归 regression → 预测的结果为连续值 | 测试样本                                 |
| 聚类 clustering                      | 监督学习 - 回归和分类的代表              |
| 无监督学习 - 聚类的代表              | 泛化能力 generalization                  |
| 假设空间                             | 版本空间                                 |

### 1.4 归纳偏好

归纳偏好：在学习过程中算法对某种类型的假设的偏好。

**阿卡姆剃刀**同来引导算法确立正确的偏好：若有多个假设与观察一致，则选最简单的那个。

归纳偏好对应了学习算法本身的所做出的关于“什么样的模型更好”的假设。在具体的现实问题中，这个假设是否成立，即算法的归纳偏好是否与问题本身匹配，大多数时候直接决定了算法是否能够取得好的性能。

![image-20200107210313260](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20200107210313260.png)

上图，A曲线为算法A学习出来的模型，B曲线为算法B学习出来的模型。从图1.4(a)能够看出，测试样本几乎都落在了A曲线上，而没有测试样本落在B曲线上，这说明了算法A的泛化能力更强。

但是，即使A看起来更加优秀，这不代表在所有情况下A都能比B强。如图1.4(b)，B强于A。这就是“没有免费的午餐定理”，简称NFL：无论A显得多么优秀，B看起来多么笨拙，他们的期望性能在理论上是一致的。

我们必须得结合实际情况去比较算法的性能，NFL只是说明在所有情况下，不同算法性能相同，但是我们解决问题时，往往只是一个方面，不会考虑所有情况。所以NFL的寓意在于，脱离具体问题去空泛地谈算法性能毫无意义。**学习算法自身的归纳偏好与问题是否相匹配**，往往会起到决定性作用。

### 1.5 历史发展

推理期→知识期→学习期



## 第二章 模型评估与选择

### 2.1 经验误差与过拟合

通常我们讲分类错误的样本数占总样本数的比例成为**错误率**。

m个样本中有a个样本分类错误，则错误率E为：
$$
E = \frac{a}{m}
$$
精度：$$1-a/m$$

学习器的实际预测输出与样本的真实输出之间的差异称为“误差”，学习器在训练集上的误差成为“训练误差”或者“经验误差”，在新样本上的误差成为“泛化误差“。我们希望得到泛化误差小的学习器。然而我们事先并不知道新样本是什么样，实际上能做的就是努力使经验误差最小化。在很多情况下，我们能够学得一种分类错误为零的学习器，但是这不是我们想要的。这种学习器多数情况下都不好。

**过拟合**overfitting：学习器把训练样本学得太好，将训练样本本身的一些特点当作了所有潜在样本的一般性质，即没有正确地学习到所有潜在样本普遍的性质。

**欠拟合**underfitting：对于训练样本的一般性质尚未学好。

![image-20200107213811272](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20200107213811272.png)

有很多种因素能够导致过拟合。其中一种常见的现象是由于学习能力过于强大，以至于把训练样本所包含的不太一般的性质都学到了。欠拟合是学习能力低下而造成的。欠拟合容易解决，神经网络学习增加训练轮数就可以。但是过拟合很棘手，是机器学习面临的关键障碍。各类学习方法都有对应的措施。



在现实任务中，我们往往有队中学习算法可供选择，对于同一个算法，采用不同的参数配置，也会产生不同的模型，应该使用哪种学习算法、参数配置？这就是**模型选择**问题。

### 2.2 评估方法 

利用实验测试对泛化误差进行评估。使用测试集测试学习器对新样本的判别能力，利用测试误差来近似泛化误差。**测试集尽量和训练集互斥**，即测试样本尽量不在训练集中出现、未在训练过程中使用。

如何从数据集D中得到训练集S和测试集D？

#### 2.2.1 留出法

“**留出法**”直接将数据集D划分未两个互斥的集合，其中一个未训练集S，另一个作为测试集T。在S上训练出模型后，用T来评估其测试误差，作为对泛化误差的估计。

#### 2.2.2 交叉验证法

“交叉验证法”先将数据集D划分为k个大小相似的互斥子集。每个子集$D_i$都尽可能保持数据分布的一致性，即从D中通过分层采样得到。然后，**每次用$k-1$个子集作为训练集，余下那个子集作为测试集**；这样就可获得k组训练/测试集，从而可进行k次训练和测试，最终返回的是这k个测试结果的均值。显然，交叉验证法评估结果的稳定性和保真性在很大程度上取决于k的取值，该方法又称“k折交叉验证”。k最常用的取值是10，此时称为10折交叉验证。

#### 2.2.3 自助法

我们希望评估的是用D训练出的模型。但在留出法和交叉验证法中，由于保留了一部分样本用于测试，因此实际评估的模型所使用的训练集比D小，这必然会引入一些因训练样本规模不同而导致的估计偏差。

“自助法”是一个比较好的解决方案，它直接以自助采样法(bootstrap sampling)为基础。给定包含m个样本的数据集D，我们对它进行采样产生数据集$D'$：每次随机从D中挑选一个样本，将其拷贝放入$D'$，然后再将该样本放回初始数据集D中，使得该样本在下次采样时仍有可能被采到；这个过程重复执行m次后，我们就得到了包含m个样本的数据集$D'$，这就是自助采样的结果。通过计算，初始数据集D中约有36.8%的样本未出现在采样数据集$D'$中。于是我们将$D'$作文训练集，$D\D'$作测试集。这样，实际评估的模型与期望评估的模型都使用m个训练样本，而我们仍有数据总量约1/3的、没在训练集中出现的样本用于测试。这样的测试结果，亦称“包外估计”。

自助法在**数据集较小、难以有效划分训练/测试集**时很有用；此外，自助法能从初始数据集中产生多个不同的训练集，这对集成学习等方法又很大的好处。然而，自助法产生的数据集改变了初始数据集的分布，这会引入估计偏差。**因此，在初始数据量足够时，留出法和交叉验证法更常用一些。



#### 2.2.4 调参与最终模型

调参parameter tuning

给定包含m个样本的数据集D，在模型评估与选择的过程中，由于需要留出一部分数据进行评估测试，事实上我们只使用了一部分数据训练模型。**因此，在模型选择完成后，学习算法和参数配置已选定，此时应该用数据集D重新训练模型**。这个模型训练过程中使用了全部的m个样本，这才是我们交给用户的模型。

另外，我们通常把学得模型在实际使用中遇到的数据称为测试数据，为了加以区分，模型评估与选择中用于评估测试的数据集常成为**验证集**。例如，在研究对比不同算法的泛化性能时，我们用测试集上的判别效果来估计模型在实际使用时的泛化能力，**而把训练数据另外划分为训练集和验证集**，**基于验证集上的性能用来进行模型选择和调参。**

### 2.3 性能度量

**性能度量**是衡量模型泛化能力的评价标准。

性能度量反映了任务需求，在对比不同模型的能力时，使用不同的性能度量往往会导致不同的评判结果。

对于给定样例集 $$D = {(x_1,y_1),(x_2,y_2),...,(x_m,y_m)}$$，其中$$y_i$$是示例$$x_i$$的真实标记。要评估学习器f的性能，就要把学习器预测结果$$f(x)$$与真实标记$$y$$进行比较。

回归任务最常用的性能度量是均方误差。

$$E(f;D)=\frac{1}{m}\sum_{i=1}^m(f(x_i)-y_i)^2$$

#### 2.3.1 错误率与精度

错误率和精度是分类任务中最常用 的两种性能度量，既适用于二分类任务，也适用于多分类任务。

#### 2.3.2 查准率、查全率与F1

错误率与精度虽常用，但有时候并不满足某些任务需求。查准率(precision)和查全率(recall)。

查准率：在挑出来的”好“西瓜中，有多是真的好西瓜（旨在追求结果的准确度，往往挑拣越仔细，准度越高）

查全率：在所有好西瓜中，有多少被挑出来了（旨在追求更多的正确的预测，往往挑拣越多，更多的正确预测）

根据二分类问题，可将样例根据其真实类别与学习器预测类别的组合划分成TP, FP, TN, FN。

显然，TP+FP+TN+FN = 样例总数。

下面是分类结果的混淆矩阵confusion matrix

<img src="C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20200107223101182.png" alt="image-20200107223101182" style="zoom:50%;" />

若真实情况是正例，预测结果也是正例，则为TP；

若真实情况是正例，而预测结果为反例，则为FN;

其他同理，可以看出，第一个字母代表预测结果是否与真实情况一致。若一致，则为T；不一致为F；第二个字母代表预测结果，P为正例，N为反例。

查准率：

$$P=\frac{TP}{TP+FP}$$

分子为预测正确的正例的个数，分母为预测的正例的个数，代表了在预测所有正例中，正确的正例的比例。

查全率：

$$R=\frac{TP}{TP+FN}$$

分子为被预测正确的正例个数，分母为真实情况的正例个数，代表了有多少正例被正确地预测了。

P和R往往是一对矛盾的度量。P高R低，P低R高。

P-R曲线

<img src="C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20200107224737135.png" alt="image-20200107224737135" style="zoom:67%;" />

图中曲线A对应学习器A的PR曲线，该曲线包围了C，则可以认为学习器A的性能优于学习器C。

平衡点，P=R，A平衡点大于B的平衡点，则可认为学习器A性能大于学习器B。

F1度量 F1-Score 是P和R的调和平均 Harmonic mean

$$F1=\frac{2PR}{P+R}=\frac{2TP}{样例总数+TP-TN}$$



很多时候我们有多个二分类混淆矩阵，例如进行多次测试/训练，每次得到一个混淆矩阵；或者是在多个数据集上训练/测试。我们希望在n个二分类混淆矩阵上综合考察查准率和查全率。

一种直接做法就是将所有P和R求平均值，这样就得到宏查准率macro-P，宏查全率macro-R，宏F1macro-F1。

还可以先计算TP, FP, TN, FN的平均值，在计算micro-F1。



#### 2.3.3 ROC与AUC

很多学习器是为测试样本产生一个实值或者概率预测，然后将这个预测值与一个分类阈值进行比较，若大于阈值则分为正类，否则为反类。例如，神经网络在一般情况下是对每个测试样本预测出一个[0.0,1.0]之间的实值，然后将这个值与0.5进行比较，大于0.5为正例，小于0.5为反例。实际上，我们根据这个输出的实值对测试样本进行排序，最可能是正例的排在最前面。我们在中间某个地方设置截断点将样本分为两部分，前一部分是正例，后一部分是反例。

若我们更重视查准率，我们应该将截断点设置在靠后位置；重视查全率，反之亦然。因此，排序本身的质量好坏，体现了综合考虑学习器在不同任务下的”期望泛化性能“的好坏。ROC曲线真实从这个角度来研究学习器泛化性能的有力工具。

ROC全称是受试者工作特征(Receiver Operating Characteristic)曲线。横坐标是TPR真正例率，纵坐标是FPR假正例率。

$$TPR=\frac{TP}{TP+FN}$$ 分母为真实情况正例的个数，分子为正确预测的正例

$$FPR=\frac{FP}{TN+FP}$$ 分母为真实情况反例的个数，分子为错误预测的反例

![image-20200107231301541](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20200107231301541.png)

如上图，2.4(a)对应于将所有正例排在所有反例之前的理想模型，2.4(b)对应利用有限个测试样例来绘制的ROC图。ROC图绘制过程：

给定m^+^个正例和m^-^个反例，根据学习器预测结果对样例进行排序，然后把分类阈值设为最大，即把所有样例均预测为反例，此时真正例率和假反例率均为0，在坐标$$(0,0)$$处标记一点。

然后，将分类阈值依次设为每个样例的预测值，即依次将每个样例划分为正例。设前一个标记点坐标为$$(x,y)$$，当前若为真正例，则对应标记标记点的坐标为$$(x,y+\frac{1}{m^+})$$，因为此时增加一个真正例，假正例率不变，x不变，真正例率增加，$$TPR=\frac{TP+1}{TP+1+FN-1}=\frac{TP}{TP+FN}+\frac{1}{TP+FN}$$ ；同理，当前若为假正例，则对应标记点坐标为$$(x+\frac{1}{m^-},y)$$，然后用线段连接相邻点即可。



进行学习器比较时，与PR图相似，若一个学习器的ROC曲线被另一个学习器的曲线完全包住，则可断言后者的性能由于前者；若两个学习器的ROC曲线发生交叉，难以判断孰优孰劣。此时，较为合理的判断依据是比较ROC曲线下的面积，即AUC(Area Under ROC Curve)。

#### 2.3.4 代价敏感错误率与代价曲线

在现实任务中，不同类型的错误造成的后果不同，代价不同，这就是“非均等代价”unequal cost。

“代价矩阵”：$$cost_{ij}$$是指将第i类样本预测为第j类样本的代价。一般来说，$$cost_{ii}=0$$；若将第0类判别为第1类所造成的损失更大，则$$cost_{01}>cost_{10}$$。

<img src="C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20200108124934997.png" alt="image-20200108124934997" style="zoom:50%;" />

前所提到的性能度量，它们大都隐式假设了均等代价。在非均等代价下，我们所希望的不再是简单地最小化错误次数，而是希望最小化“总体代价”。若将上述表格中第0类作为正类，第1类作为反类，令$$D^+$$与$$D^-$$分别代表样例集$$D$$中的正例子集和反例子集，则“代价敏感错误率”为：
$$E(f;D;cost)=\frac{1}{m}(\sum_{x_i\in{D^+}}\prod(f(x_i)\neq{y_i})\times{cost_{01}}+\sum_{x_i\in{D^-}}\prod{(f(x_i)\neq{y_i})\times{cost_{10}}})$$

在非均等代价下，ROC曲线不能直接反映出学习器的期望总体代价，而“代价曲线”则可以达到给目的

。代价曲线中的每条线段对应ROC曲线的一个点，利用ROC曲线上所有点画出代价曲线的所有线段，然后取所有线段的下界，围成的面积即为所有条件下学习器的期望总体代价，如下图所示。

![image-20200108130652131](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20200108130652131.png)

### 2.4 比较检验

有了实验评估方法和性能度量，看起来就能对学习器的性能进行评估比较了。如何进行比较比想象中要复杂的多。这里面涉及几个重要的因素：首先，我们希望比较的是泛化性能，然而通过实验评估方法我们获得的是测试集上的性能，两者的对比结果可能未必相同（？）；第二，测试集上的性能与测试集本身的选择有很大的关系，即便用相同大小的测试集，若包含的测试样例不同，测试结果也会不同；第三，机器学习算法本身有一定的随机性，即便用相同的参数设置在同一个测试集上多次运行，其结果也会有不同。

统计假设检验为我们进行学习器性能比较提供了重要依据。基于假设检验结果我们可推断出，若在测试集上观察到学习器A比B好，则A的泛化性能是否在统计意义上优于B，以及这个结论的把握有多大。下面将介绍两种最基本的假设检验，然后介绍几种常用的机器学习性能比较方法。本节默认以错误率为性能度量，用$$\epsilon$$表示。

#### 2.4.1 假设检验



#### 2.4.2 交叉验证t检验



#### 2.4.3 McNemar检验



#### 2.4.4 Friedman检验与Nemenyi后续检验



### 2.5 偏差与方差

对学习算法除了通过实验估计其泛化性能，人们往往还希望了解它为什么具有这样的性能。“偏差-方差分解”是解释学习算法泛化性能的一种重要工具。

偏差-方差分解对学习算法的期望泛化错误率进行拆解，经过推导，泛化误差可分解为偏差、方差与噪声之和。

$$E(f;D)=bias^2(x)+var(x)+\varepsilon^2$$

其中**偏差**为期望输出与真实标记的差别：

$$bias^2(x)=(\overline{f}(x)-y)^2$$

**噪声**为：（$$y_D$$为x在数据集中的标记，$$y$$为$$x$$的真实标记，由于噪声的存在，有可能$$y_D\neq{y}$$）

$$\varepsilon^2=E_D[(y_D-y)^2]$$

**偏差**度量了学习算法的期望预测与真实结果的偏离程度，即刻画了**学习算法本身的拟合能力**；

**方差**度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了**数据扰动所造成的影响**；

**噪声**则表达了在当前任务上任何学习算法所能够达到的期望泛化误差的下届，即刻画了**学习问题本身的难度**。

偏差-方差分解说明，泛化性能是由学习算法的能力、数据的充分性以及学习任务本身的难度所共同决定的。给定学习任务，为了取得好的泛化性能，则需使偏差较小，即能够充分拟合数据，并且使方差较小，即使得数据扰动产生的影响小。

一般来说，偏差与方差是有冲突的。偏差-方差窘境。给定学习任务，假定我们能控制学习算法的训练程度，则在训练不足时，学习器拟合能力不够强，训练数据的扰动不足以使学习器产生显著变化，此时偏差主导了泛化错误率；随着训练程度的加深，学习器的你和能力逐渐增强，训练数据发生的扰动渐渐能被学习器学到，方差逐渐主导了泛化错误率；在训练充足后，学习器的拟合能力已非常强，训练数据发生轻微扰动都会导致学习器产生显著变化，若训练数据自身的、非全局的性质被学习器学到了，则将发生过拟合。

<img src="C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20200108134152840.png" alt="image-20200108134152840" style="zoom:67%;" />



## 第三章 线性模型





## 第四章 决策树



## 第五章 神经网络

### 5.1 神经元模型

神经网络的定义：神经网络是由具有适应性的简单单元组成的广泛并行互联的网络，它的组织能够模拟生物神经系统对真实世界物体所作出的交互式反应。

神经网络中最基本的成分是神经元模型。

M-P神经元模型一直沿用至今，如下图所示。

<img src="C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20200108204359926.png" alt="image-20200108204359926" style="zoom:67%;" />

在这个模型中，神经元接收到来自n个其他神经元传递过来的输入信号，这些输入信号通过带权重的连接进行传递，神经元接收到的总的输入值将于神经元的阈值进行比较，然后通过激活函数(activation function)处理以产生神经元的输出。

理想中的激活函数是图5.2(a)所示的阶跃函数，它将输入值映射为输出值“0”或“1”，显然“1”对神经元兴奋，“0”对神经元抑制。然而阶跃函数具有不连续、不光滑等不太好的性质，因此实际上常用Sigmoid函数作为激活函数。

![image-20200108204912656](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20200108204912656.png)

它把在较大范围内变化的输入值挤压到(0,1)输出值范围内，因此有时也成为挤压函数。

### 5.2 感知机与多层网络

感知机(Perceptron)由两层神经元组成，如下图所示。输入层接收外界输入信号后传递给输出层，输出层是M-P神经元，亦称阈值逻辑单元threshold logic unit。

<img src="C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20200108205437439.png" alt="image-20200108205437439" style="zoom:67%;" />

图中，$$y=f(w_1x_1+w_2x_2-\theta)$$，假定$$f$$是阶跃函数，$$\theta$$是阈值。

感知机能够容易地实现逻辑与、或、非运算，通过改变连接权值和$$\theta$$的大小。

给定训练数据集，权重$$w_i(i=1,2,...,n)$$以及阈值$$\theta$$可通过学习得到。权重和阈值的学习可统一为权重的学习。感知机学习规则是对训练样例$$(x,y)$$，若当前感知机的输出为$$\hat{y}$$，则感知机权重将这样调整：

$$w_i←w_i+\Delta{w_i}$$

$$\Delta{w_i}=\eta(y-\hat{y})x_i$$

其中$$\eta\in{(0,1)}$$成为学习率。显然，如果感知机对训练样例$$(x,y)$$预测正确，即$$\hat{y}=y$$，则感知机不发生变化，否则将根据错误的程度进行权重调整。根据第二个式子，当预测结果跟实际情况差距越大，权重调整的幅度越大，预测结果近似于实际情况，则权重只需要很小的调整。

需要注意的是，感知机只有输出层神经元进行激活函数处理，即只拥有一层功能神经元，其学习能力非常有限。对于线性可分的模式，感知机的学习过程一定会收敛而球的适当的权向量；对于非线性可分的问题，感知机学习过程将会发生震荡，**w**难以稳定下来，不能求得合适解，例如异或问题。

要解决非线性可分问题，需要考虑使用多层工能神经元。如图5.5的这个简单两层感知机就能解决异或问题。在图5.5(a)中，输出层与输入层之间的一层神经元，被称为隐含层hidden layer。隐含层和输出层神经元都是拥有激活函数的功能神经元。

![image-20200108211534106](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20200108211534106.png)

更一般的，常见的神经网络如图5.6所示的层级结构，每层神经元与下一层神经元全互联，神经元之间不存在同层连接，也没有跨层连接。这样的神经网络结构通常称为“多层前馈神经网络”multi-layer feedforward neural networks

![image-20200108212002400](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20200108212002400.png)

其中输入层神经元接收外界输入，隐含层与输出层神经元对信号进行加工，最终结果由输出层神经元输出。输入层神经元仅仅是接受输入，不进行函数处理，隐含层与输出层包含功能神经元。因此图5.6(a)通常被称为“两层网络”。神经网络的学习过程，就是根据训练数据来调整神经元之间的“连接全”以及每个功能神经元的阈值。

### 5.3 误差逆转传播算法（反向传播算法）

多层网络的学习能力比单层强得多，欲训练多层网络，单层感知机学习规则显然不够用了，需要更强大的学习算法。误差逆传播(error Backpropagation，简称BP)，迄今最成功的神经网络学习算法。BP算法不仅可以用于多层前馈神经网络，还可以用于其他类型的神经网络，例如训练递归神经网络。但通常说BP网络时，一般是指用BP算法训练的多层前馈神经网络。

给定训练集$$D={(\boldsymbol{x_1},\boldsymbol{y_1}),(\boldsymbol{x_2},\boldsymbol{y_2}),...,(\boldsymbol{x_m},\boldsymbol{y_m})},\boldsymbol{x_i}\in{R^d},\boldsymbol{y_i}\in{R^l}$$

即输入示例由$$d$$个属性描述，输出$$l$$维实向量。

![image-20200108214256907](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20200108214256907.png)

图5.7中给出了一个拥有d个输入神经元、l个输出神经元，q个隐层神经元的多层前馈网络结构。

记隐层第h个神经元接收到的输入为$$\alpha_h=\sum_{i=1}^dv_{ih}x_i$$

输出层第j个神经元接收到的输入为$$\beta_j=\sum_{h=1}^qw_{hj}b_h$$

其中$$b_h$$为隐层第h个神经元的输出。假设激活函数都是Sigmoid。

BP是一个迭代学习算法，在迭代的每一轮中采用广义的感知机学习规则对参数进行更新估计，任意参数v的更新估计式为

$$v←v+\Delta{v}$$

BP算法基于梯度下降的策略，以目标的负梯度方向对参数进行调整。



学习率$$\eta$$控制着算法每一轮迭代中的更新步长，若太大则容易震荡，太小则收敛速度又会过慢。要适当改变学习率。

BP算法的工作流程：先将输入示例提供给输入层神经元，然后逐层将信号前传，直到产生输出层的结果；然后计算输出层的误差，再将误差逆向传播至隐层神经元，最后根据隐层神经元的误差来对连接权和阈值进行调整。该迭代过程循环进行，直到达到某些停止条件为止，例如训练误差已达到一个很小的值。

![image-20200108220220412](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20200108220220412.png)

以上就是标准BP算法，是基于单个训练样例更新连接权和阈值。还有累积误差逆传播算法。通常，标准BP算法每次更新只针对单个样例，参数更新得非常频繁，而且对不同样例进行更新的效果可能出现“抵消现象”。因此，为了达到同样的累积误差最小化，标准BP算法往往需要进行更多次的迭代。累积BP算法直接针对累积误差最小化，它在读取整个训练集D一遍后才对参数进行更新，更新频率低得多。但在很多任务中，累积误差下降到一定程度之后，进一步下降会非常缓慢，这时标准BP往往会更快获得较好的解，尤其是在训练集D非常大时更明显。

Hornik证明，只需一个包含足够都神经元的隐层，多层前馈网络就能以任意精度逼近任意复杂度的连续函数，然而，如何设置隐层神经元的个数仍是个未解决的问题，实际应用中通常靠试错法调整。

强大的表示能力，BP神经网络经常遭遇过拟合，其训练误差持续降低，但测试误差却可能上升。有两种策略常用来缓解BP网络的过拟合。

第一种是早停：将数据集分成训练集和验证集，训练集用来计算梯度、更新连接权和阈值，验证集用来估计误差，若训练集误差降低但验证集误差升高，则停止训练，同时返回具有最小验证集误差的连接权和阈值。

第二种是正则化regularization，其基本思想是在误差目标函数中增加一个用于描述网络复杂度的部分，例如连接权和阈值的平方和。改变误差目标函数。

### 5.4 全局最小与局部极小

若用$$E$$表示神经网络在训练集上的误差，则它显然是关于连接权$$w$$和阈值$$\theta$$的函数。此时，神经网络的训练过程可看作一个参数寻优的过程，即在参数空间中，寻找一组最优参数使得$$E$$最小。

我们常会谈到两种最优：局部极小和全局最小。

局部极小解是参数空间的某个点，其领域点的误差函数值均不小于概念的函数值；全局最小解则是参数弓箭中所有点的误差函数值均不小于该点的误差函数值。

基于梯度的搜索是使用最为广泛的参数寻优方法。在此类方法中，我们从某些初始解出发，迭代寻找最优参数值。每次迭代中，我们先计算误差函数在当前点的梯度，然后根据梯度确定搜索方向。例如，由于负梯度方向是函数值下降最快的方向，因此梯度下降法就是沿着负梯度方向搜索最优解。若误差函数在当前点的梯度为零，说明已经达到局部极小，更新量为零，这意味着参数的迭代更新将在此停止。如果误差函数具有多个局部最小，则不能保证这个局部极小就是全局最小。

在现实任务中，人们常采用一下策略来试图“跳出”局部极小，从而进一步接近全局最小：

- 以多组不同参数值初始化多个神经网络，按照标准方法训练后，取其误差最小的解作为最终参数。这相当于从多个不同初始点开始搜索，这样就可能陷入不同的局部极小，从中进行选择有可能获得更接近全局最小的结果。
- 使用“**模拟退火**”（simulated）技术。模拟退火在每一步都以一定的概率接受比当前解更差的结果，从而有助于“跳出”局部极小。在每步迭代的过程中，接受“次优解”的概率要随着时间的推移而逐渐降低，从而保证算法稳定。
- 使用**随机梯度下降**。与标准梯度下降法精确计算梯度不同，随机梯度下降法在梯度计算时加入了随机因素。于是，即便陷入局部极小点，它计算处出的梯度仍可能不为零，这样就有机会跳出局部极小继续搜索。

此外，**遗传算法**也常用来训练神经网络以更好地逼近全局最小。

### 5.6 深度学习

深度学习模型就是很深层的神经网络结构。通过增加隐层的数目来提升模型的复杂度，对应的神经元连接权、阈值等参数就会增加，包括激活函数嵌套的层数。然而，多隐层神经网络难以直接用经典算法（例如标准BP算法）进行训练，因为误差在多隐层内逆传播，往往会发散而不能收敛得到稳定状态。

无监督逐层训练是多隐层网络训练的有效手段，基本思想是每次训练一层隐节点，训练时将上一层隐节点的输出作为输入，而本层隐节点的输出作为下一层隐节点的输入，这称为“预训练”；在预训练全部完成后，再对整个网络进行“微调”(fine-tuning)训练。

事实上，“预训练+微调”的做法可视为将大量参数分组，对每组先找到局部看来比较好的设置，然后再基于这些局部较优的结果联合起来进行全局寻优。

另一种节省训练开销的策略是“权共享”，即让一组神经元使用相同的连接权。这个策略再CNN中发挥了重要作用。

![image-20200109134839714](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20200109134839714.png)

如图5.15所示，网络输入是一个32×32的手写数字图像，输出是其识别的结果，CNN符合多个卷积层和采样层对输入信号进行加工，然后在连接层实现与输出目标之间的映射。每个卷积层都包含多个特征映射(feature map)，**每个特征映射是一个由多个神经元构成的“平面”**，通过一种**卷积滤波器**提取输入的一种特征。图中，第一个卷积层由6个特征映射构成，每个特征映射是一个28×28的神经元阵列，其中每个神经元负责从5×5的区域通过卷积滤波器提取局部特征。采样层对卷积层输出的结果进行亚采样，每个特征映射由28×28变成了14×14。通过多层的卷积层和采样曾，CNN将原始图像映射称120维特征向量，最后通过一个由84个神经元构成的连接阶层和10维输出层完成识别任务。CNN可用BP算法进行训练，但在训练中，无论是卷积层还是采样层，**其每组神经元（即图中的每个平面）都采用相同的连接权**，从而大幅减少了需要训练的参数数目。

深度学习，通过多层处理，逐渐将初始的“低层”特征表示转化为“高层”特征表示后，用“简单模型”即可完成复杂的分类任务等学习任务。由此可将深度学习理解为“特征学习”(feature learning)或“表征学习”(representation learning)。



## 第六章 支持向量机

### 6.1 间隔与支持向量

给定训练样本集$$D=\{(\boldsymbol{x_1},y_1),(\boldsymbol{x_2},y_2),...,(\boldsymbol{x_m},y_m)\}, y_i\in{\{-1,+1\}}$$，分类学习最基本的想法就是基于训练集$$D$$在样本空间中找到一个找到一个划分超平面，将不同类别的样本分开。但能将训练样本划分的超平面可能有很多，如图6.1所示，应该选择哪一个？

![image-20200112115514365](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20200112115514365.png)

直观上看，最中间加粗的直线，应该是对训练样本局部扰动“容忍”性最好的。换言之，这个划分超平面所产生的分类结果是最鲁棒的，对未见示例的泛化能力最强。

**超平面**：是n维欧氏空间中余维度等于一的线性子空间，也就是必须是(n-1)维度。超平面一定经过原点。在几何体中，超平面是一维小于其环境空间的子空间。如果空间是三维的，那么它的超平面是二维平面，如果空间是二位的，则其超平面是一维线。

在样本空间中，划分超平面可通过如下线性方程来描述：

$$\boldsymbol{w^Tx}+b=0$$

其中$$\boldsymbol{w}=(w_1;w_2;...;w_d)$$为法向量，决定了超平面的方向。$$b$$为位移项，决定了超平面与原点之间的距离。显然，划分超平面可被法向量$$\boldsymbol{w}$$和位移$$b$$确定。下面我们将其记为$$(\boldsymbol{w},b)$$。样本空间中任一点$$x$$到超平面$$(\boldsymbol{w},b)$$的距离可写为

$$r=\frac{|\boldsymbol{w^T}+b|}{||\boldsymbol{w}||}$$

假设超平面$$(\boldsymbol{w},b)$$能够训练样本正确分类，即对于$$(\boldsymbol{x_i},y_i)\in{D}$$，若$$y_i=+1$$，则有$$\boldsymbol{w^Tx}+b>0$$；若$$y_i=-1$$则有$$\boldsymbol{w^Tx}+b<0$$。令

$$\boldsymbol{w^Tx}+b\geq{+1}. y=+1       $$                                                                            (6.3)

$$\boldsymbol{w^Tx}+b\leq{-1}. y=-1$$

如图6.2所示，距离超平面最近的这几个**训练样本点使上式成立，它们被称为“支持向量”**，两个异类支持向量到超平面的距离之和为

$$\gamma=\frac{2}{||\boldsymbol{w}||}$$

它被称为**间隔**

欲寻找具有**“最大间隔”**的划分超平面，也就是找到能够满足式(6.3)中约束的参数$$w, b$$使得$$\gamma$$最大。一下就是支持向量机的基本型。

<img src="C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20200112123402367.png" alt="image-20200112123402367" style="zoom:67%;" /> （6.6）

### 6.2 对偶问题

我们希望求解上式来得到间隔划分超平面所对应的模型

$$f(\boldsymbol{x})=\boldsymbol{w^T}\boldsymbol{x}+b$$                                                                       (6.7)

其中$$\boldsymbol{w}$$和$$b$$是模型参数。注意到式(6.6)本身是一个**凸二次规划**(convex quadratic programming)问题，能直接用现成的优化计算包求解，但我们可以有更高效的办法。



### 6.7 小结

支持向量机的求解通常是借助凸优化技术。如何提高效率，是SVM能适用于大规模数据一直是研究重点。对线性核SVM已有很多成果，例如基于割平面法的SVM具有线性复杂度，基于随机梯度下降的Pegasos速度甚至更快，而而坐标下降法则在稀疏数据上有很高的效率。非核线性SVM的时间复杂度在理论上不能低于$$O(m^2)$$，因此研究重点是设计快速近似算法，如基于采样的CVM、基于低秩逼近的Nystrom方法、基于随机傅里叶特征的方法等。最近有研究显示，当核矩阵特征值有很大差别时，Nystrom方法往往优于随机傅里叶特征方法。

支持向量机是针对二分类任务设计的，对多分类任务要进行专门的推广，对带结构输出的任务也已有相应的算法。支持向量回归的研究始于给出了一个较为全面的介绍。

核函数直接决定了支持向量机与核方法的最终性能，但遗憾的是，核函数的选择是一个未决的问题。多核学习使用多个核函数并通过学习获得其最优凸组合作用为最终的核函数，这实际上是在借助集成学习机制。





## 第七章 贝叶斯分类器

### 7.1 贝叶斯决策论 

**贝叶斯决策论是概率框架下实施决策的基本方法**。对分类任务来说，在所有相关概率都已知的情况下，贝叶斯决策论考虑如何基于这些概率和误判损失来选择最优的类别标记。下面我们以多分类任务为例来解释其基本原理。

[MLE和MAP](https://zhuanlan.zhihu.com/p/32480810 "标题")

## 第八章 集成学习

### 8.1 个体与集成

集成学习通过构建并结合多个学习器来完成学习任务，有时也被称为多分类器系统、基于委员会的学习等。

集成学习的一般结构：先产生一组“个体学习器”，再用某种策略将它们结合起来。个体学习器通常由一个现有的学习算法从训练数据产生。集成中的各个学习器类型不同，这样的集成是同质的，这种同质集成的个体学习器亦称“**基学习器**”。相应的学习算法称为“基学习算法”。集成可以包含不同类型的个体学习器，例如同时包含决策树和神经网络。

集成学习通过将多个学习器进行结合，常可获得比单一学习器显著优越的泛化性能。

**要有好的集成，个体学习器应“好而不同”，即个体学习器要有一定的“准确性”，并且要有多样性。**

通过推导可知，个体学习器的准确性和多样性本身就存在冲突。**事实上，如何产生“好而不同”的个体学习器，恰是集成学习研究的核心**。

根据个体学习器的生成方式，目前学习集成方法大致可分为两大类：

- 个体学习间存在强依赖关系、必须串行生成的序列化方法

  **Boosting**

- 个体学习器不存在强依赖关系、可同时生成的并行化方法

  **Bagging和随机森林**

### 8.2 Boosting

Boosting是一族可将弱学习器提升为强学习器的算法。这族算法的工作机制类似：先从初始训练集训练出一个基学习器，再根据基学习器的表现的表现**对训练样本分布进行调整**，**使得先前基学习做错的训练样本在后续受到更多关注**，然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，直到及学习器数目达到事先指定的值T，最终将这T个基学习器进行加权结合。

Boosting族算法最著名的代表是AdaBoost。

其算法如下图

![image-20200210193937581](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20200210193937581.png)

一共进行T次循环，每次循环，都会根据上一个分类器的误差来调整这一次的训练集分布，根据新的训练集分布以及新的权重来训练新的分类器。一共能够生成T个分类器，并且通过加权合成T个基学习器的线性组合$H(\boldsymbol{x})$。

Boosting算法要求基学习器能对特定的数据分布进行学习，这可通过“重赋权法”实施，即在训练过程的每一轮中，根据样本分布为每个训练样本重新赋予一个权重。对于无法接受带权样本的基学习算法，则可通过“重采样法”来处理，即在**每一轮学习中，根据样本分布队训练集重新进行采样，再用重采样而得的样本集对基学习器进行训练。**需要注意的是，Boosting算法在训练的每一轮都要检查当前生成的基学习器是否满足基本条件（检查当前基分类器是否比随机猜测好），一旦条件不满足，当前基学习器即被抛弃，且学习过程停止。在此种情况下，初始设置的学习轮数T也许还远远未达到，可能导致最终集成只包含很少的基学习器而性能不佳。使用重采样法时，可以重启动来避免训练过早停止，即在抛弃不满足条件的当前基学习器之后，可根据当前分布重新对样本进行采样，再基于新的采样结果重新训练出基学习器，从而使得学习过程持续到预设的T轮完成。

从偏差-方差分解的角度看，Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成。

### 8.3 Bagging与随机森林

**欲得到泛化性能强的集成，集成中的个体学习器应尽可能相互独立；虽然“独立”在现实任务中无法做到，但可以设法使基学习器尽可能具有较大的差距**。给定一个训练数据集，一种可能的做法是对训练样本进行采样，产生出若干个不同的子集，再从每个数据子集中训练出一个基学习器。这样，由于训练书不同，我们获得的**基学习器会具有比较大的差异**。然而，为了获得好的集成，我们同时还希望**个体学习器不能太差**。如果采样出的每个子集都完全不同，则每个基学习器只用到了一小部分训练数据，不足以有效学习，这显然无法确保产生出比较好的基学习器。我们可考虑使用相互交叠的采样子集。

#### 8.3.1 Bagging

Bagging是并行式集成学习方法最著名的代表。它直接基于自主采样法。给定包含m个样本数据集，有放回的进行采样，得到大小为m的数据集。初始训练集中有的样本出现在采样集里，有些则没有。

我们可以采样出T个不同的大小为m的采样集，然后基于每个采样集训练出一个基学习器，再将这些基学习器进行结合。在对预测输出进行结合时，Bagging通常对分类任务使用简单投票法，对回归任务使用简单平均法。若预测时出现两个类收到同样票数的情形，则随机输出任意一个，也可以进一步考察学习器投票的置信度来确定最终胜者。

Bagging是一个很高效的集成学习算法。AdaBoost只适用于二分类任务，而Bagging能不经修改地用于多分类、回归等任务。

另外，自主采样法过程给Bagging带来另一个优点：由于每个基学习平均只使用了初始训练集约63.2%的样本，剩下越36.8%的样本可用作验证集来对泛化性能进行“包外估计”out-of-bag estimate。为此需记录每个基学习器所使用的训练样本。

从偏差-方差分解的角度，Bagging主要关注降低方差，因此它在不剪枝决策树、神经网络等易受样本扰动的学习器上效用更为明显。

#### 8.3.2 随机森林

RF在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。



### 8.4 结合策略

学习器结合可能会从三个方面带来好处：

- 从统计方面看，由于学习任务的假设空间往往很大，可能有多个假设在训练集上达到同等性能，此时若使用但学习器可能因为误选而导致泛化性能不佳。
- 从计算的方面来看，学习算法往往会陷入局部极小点，通过多次运行，可能避免这种情况。
- 从表示方面来看，某些学习任务的真是假设可不在当前算法所考虑的假设空间中，此时若使用单学习器肯定无效，而通过结合多个学习器，由于相应的假设空间有所扩大，有可能学得更好的额近似。

假定集成包含T个基学习器$\{h_1,h_2,...,h_T\}$，其中$h_i$在示例$\boldsymbol{x}$上的输出为$h_i(\boldsymbol{x})$。下面将介绍几种对$h_i$进行结合的常见策略。

#### 8.4.1 平均法



#### 8.4.2 投票法



#### 8.4.3 学习法



### 8.5 多样性

#### 8.5.1 误差-分歧分解



#### 8.5.2 多样性度量



#### 8.5.3 多样性增强

- 数据样本扰动
- 输入属性扰动
- 输出表示扰动
- 算法参数扰动



## 第九章 聚类



## 第十章 降维与度量学习



## 第十一章 特征选择与稀疏学习



## 第十二章 计算学习理论



## 第十三章 半监督学习



## 第十四章 概率图模型



## 第十五章 规则学习



## 第十六章 强化学习



